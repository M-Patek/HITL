import os
import logging
import asyncio
from functools import partial
from typing import List, Dict, Any, Optional
# å‡è®¾ä½¿ç”¨ google.generativeai æˆ–å…¶ä»–æ–¹å¼è·å– embedding
import google.generativeai as genai 

try:
    from pinecone import Pinecone
except ImportError:
    Pinecone = None

logger = logging.getLogger("Tools-Memory")

class VectorMemoryTool:
    """
    [Protocol Phase 3 Enhanced]
    æ”¯æŒè¯­ä¹‰ç¼“å­˜ (Semantic Caching) çš„å‘é‡è®°å¿†å·¥å…·ã€‚
    å·²å…¨é¢å¼‚æ­¥åŒ– (Async I/O non-blocking)ã€‚
    """
    def __init__(self, api_key: str, environment: str, index_name: str):
        # æ£€æŸ¥æ˜¯å¦å…·å¤‡å¯ç”¨æ¡ä»¶
        self.enabled = bool(api_key and index_name and Pinecone)
        self.index = None
        if self.enabled:
            try:
                self.pc = Pinecone(api_key=api_key)
                self.index = self.pc.Index(index_name)
            except Exception as e:
                logger.error(f"Pinecone init failed: {e}")
                self.enabled = False
        else:
            logger.warning("Pinecone not configured. Memory & Caching disabled.")

    def _get_embedding_sync(self, text: str) -> List[float]:
        """åŒæ­¥è·å–åµŒå…¥ (å†…éƒ¨ Helper)"""
        if not text: return []
        try:
            # è¿™é‡Œçš„ model éœ€ä¸æ‚¨çš„ Pinecone index ç»´åº¦ä¸€è‡´ (e.g., 768)
            result = genai.embed_content(
                model="models/text-embedding-004",
                content=text,
                task_type="retrieval_query"
            )
            return result['embedding']
        except Exception as e:
            logger.error(f"Embedding failed: {e}")
            return []

    async def _get_embedding(self, text: str) -> List[float]:
        """å¼‚æ­¥è·å–åµŒå…¥ (Non-blocking wrapper)"""
        loop = asyncio.get_running_loop()
        return await loop.run_in_executor(None, self._get_embedding_sync, text)

    async def check_semantic_cache(self, query: str, threshold: float = 0.95) -> Optional[str]:
        """
        [Phase 3] è¯­ä¹‰ç¼“å­˜å‘½ä¸­æ£€æŸ¥ (Async)
        """
        if not self.enabled or not self.index: return None

        try:
            # 1. è·å–å‘é‡ (Async)
            vector = await self._get_embedding(query)
            if not vector: return None

            # 2. æŸ¥è¯¢ Pinecone (Run in Executor)
            loop = asyncio.get_running_loop()
            
            # å®šä¹‰åŒæ­¥æŸ¥è¯¢å‡½æ•°
            def _query_pinecone():
                return self.index.query(
                    vector=vector,
                    top_k=1,
                    include_metadata=True,
                    filter={"type": "cache_entry"} 
                )
            
            response = await loop.run_in_executor(None, _query_pinecone)

            if response and response.matches:
                match = response.matches[0]
                if match.score >= threshold:
                    logger.info(f"âš¡ï¸ [Cache Hit] Query: '{query[:20]}...' (Score: {match.score:.4f})")
                    return match.metadata.get("response_text")
        
        except Exception as e:
            logger.warning(f"Cache lookup failed: {e}")
            
        return None

    async def store_cache(self, query: str, response: str):
        """å°† LLM çš„é—®ç­”å¯¹å­˜å…¥ç¼“å­˜ (Async)"""
        if not self.enabled or not self.index: return
        try:
            vector = await self._get_embedding(query)
            if vector:
                loop = asyncio.get_running_loop()
                def _upsert_cache():
                    self.index.upsert(vectors=[{
                        "id": f"cache-{hash(query)}",
                        "values": vector,
                        "metadata": {
                            "type": "cache_entry",
                            "query_text": query,
                            "response_text": response
                        }
                    }])
                await loop.run_in_executor(None, _upsert_cache)
        except Exception as e:
            logger.warning(f"Failed to store cache: {e}")

    async def store_output(self, task_id: str, content: str, agent_role: str):
        """
        å­˜å‚¨ Agent çš„äº§å‡ºåˆ°é•¿æœŸè®°å¿†ä¸­ (Async)
        """
        if not self.enabled or not self.index: 
            logger.info(f"ğŸ’¾ [Memory Mock] Storing output from {agent_role} (Pinecone Disabled)")
            return

        try:
            vector = await self._get_embedding(content)
            if vector:
                loop = asyncio.get_running_loop()
                def _upsert_memory():
                    self.index.upsert(vectors=[{
                        "id": f"mem-{task_id}-{agent_role}-{hash(content)}",
                        "values": vector,
                        "metadata": {
                            "type": "agent_output",
                            "task_id": task_id,
                            "agent": agent_role,
                            "content_snippet": content[:500]
                        }
                    }])
                await loop.run_in_executor(None, _upsert_memory)
                logger.info(f"ğŸ’¾ [Memory] Saved output from {agent_role}")
        except Exception as e:
            logger.error(f"Failed to store output: {e}")
