import os
import logging
from typing import List, Dict, Any, Optional
# å‡è®¾ä½¿ç”¨ google.generativeai æˆ–å…¶ä»–æ–¹å¼è·å– embedding
import google.generativeai as genai 

try:
    from pinecone import Pinecone
except ImportError:
    Pinecone = None

logger = logging.getLogger("Tools-Memory")

class VectorMemoryTool:
    """
    [Protocol Phase 3 Enhanced]
    æ”¯æŒè¯­ä¹‰ç¼“å­˜ (Semantic Caching) çš„å‘é‡è®°å¿†å·¥å…·ã€‚
    """
    def __init__(self, api_key: str, environment: str, index_name: str):
        # æ£€æŸ¥æ˜¯å¦å…·å¤‡å¯ç”¨æ¡ä»¶
        self.enabled = bool(api_key and index_name and Pinecone)
        if self.enabled:
            try:
                self.pc = Pinecone(api_key=api_key)
                self.index = self.pc.Index(index_name)
            except Exception as e:
                logger.error(f"Pinecone init failed: {e}")
                self.enabled = False
        else:
            logger.warning("Pinecone not configured. Memory & Caching disabled.")

    def _get_embedding(self, text: str) -> List[float]:
        """è·å–æ–‡æœ¬åµŒå…¥ (Mock or Real)"""
        if not text: return []
        try:
            # è¿™é‡Œçš„ model éœ€ä¸æ‚¨çš„ Pinecone index ç»´åº¦ä¸€è‡´ (e.g., 768)
            # æ³¨æ„ï¼šå¦‚æœå®‰è£…çš„æ˜¯ google-generativeaiï¼Œæ­¤è°ƒç”¨æ–¹å¼æ­£ç¡®
            result = genai.embed_content(
                model="models/text-embedding-004",
                content=text,
                task_type="retrieval_query"
            )
            return result['embedding']
        except Exception as e:
            logger.error(f"Embedding failed: {e}")
            return []

    def check_semantic_cache(self, query: str, threshold: float = 0.95) -> Optional[str]:
        """
        [Phase 3] è¯­ä¹‰ç¼“å­˜å‘½ä¸­æ£€æŸ¥
        åœ¨è°ƒç”¨ LLM ä¹‹å‰ï¼Œå…ˆæŸ¥åº“ã€‚å¦‚æœå‘ç°æé«˜ç›¸ä¼¼åº¦çš„é—®é¢˜ï¼Œç›´æ¥è¿”å›å†å²ç­”æ¡ˆã€‚
        """
        if not self.enabled: return None

        try:
            vector = self._get_embedding(query)
            if not vector: return None

            # æŸ¥è¯¢ Cache Namespace (å‡è®¾æˆ‘ä»¬å°† Cache å­˜åœ¨ä¸“é—¨çš„ namespace æˆ– metadataæ ‡è®°ä¸­)
            # è¿™é‡Œä¸ºäº†ç®€å•ï¼Œç›´æ¥æŸ¥å…¨å±€ï¼Œé€šè¿‡ metadata.type='cache_entry' è¿‡æ»¤
            response = self.index.query(
                vector=vector,
                top_k=1,
                include_metadata=True,
                filter={"type": "cache_entry"} 
            )

            if response and response.matches:
                match = response.matches[0]
                if match.score >= threshold:
                    logger.info(f"âš¡ï¸ [Cache Hit] Query: '{query[:20]}...' (Score: {match.score:.4f})")
                    return match.metadata.get("response_text")
        
        except Exception as e:
            logger.warning(f"Cache lookup failed: {e}")
            
        return None

    def store_cache(self, query: str, response: str):
        """å°† LLM çš„é—®ç­”å¯¹å­˜å…¥ç¼“å­˜"""
        if not self.enabled: return
        try:
            vector = self._get_embedding(query)
            if vector:
                self.index.upsert(vectors=[{
                    "id": f"cache-{hash(query)}",
                    "values": vector,
                    "metadata": {
                        "type": "cache_entry",
                        "query_text": query,
                        "response_text": response
                    }
                }])
        except Exception as e:
            logger.warning(f"Failed to store cache: {e}")

    def store_output(self, task_id: str, content: str, agent_role: str):
        """
        [Fix] å­˜å‚¨ Agent çš„äº§å‡ºåˆ°é•¿æœŸè®°å¿†ä¸­ã€‚
        ä¹‹å‰ agents.py è°ƒç”¨äº†è¿™ä¸ªä¸å­˜åœ¨çš„æ–¹æ³•ï¼Œç°åœ¨æˆ‘ä»¬è¡¥å…¨å®ƒã€‚
        """
        if not self.enabled: 
            # å¦‚æœæ²¡å¯ç”¨ï¼Œä»…æ‰“å°æ—¥å¿—
            logger.info(f"ğŸ’¾ [Memory Mock] Storing output from {agent_role} (Pinecone Disabled)")
            return

        try:
            vector = self._get_embedding(content)
            if vector:
                self.index.upsert(vectors=[{
                    "id": f"mem-{task_id}-{agent_role}-{hash(content)}",
                    "values": vector,
                    "metadata": {
                        "type": "agent_output",
                        "task_id": task_id,
                        "agent": agent_role,
                        "content_snippet": content[:500]
                    }
                }])
                logger.info(f"ğŸ’¾ [Memory] Saved output from {agent_role}")
        except Exception as e:
            logger.error(f"Failed to store output: {e}")
