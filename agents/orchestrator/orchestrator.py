from typing import Dict, Any, Literal, List
from pydantic import BaseModel
from core.rotator import GeminiKeyRotator
from core.models import ProjectState
from config.keys import GEMINI_MODEL_NAME

class SupervisorDecision(BaseModel):
    next_agent: Literal["researcher", "coding_crew", "data_crew", "content_crew", "FINISH"]
    instruction: str
    reasoning: str

class OrchestratorAgent:
    """
    [SWARM 2.0] Orchestrator Agent
    å…·å¤‡æ— é™è®°å¿†ç®¡ç†èƒ½åŠ›çš„å¤šæ¨¡æ€è¶…çº§è°ƒåº¦å™¨ã€‚
    """
    def __init__(self, rotator: GeminiKeyRotator, system_instruction: str):
        self.rotator = rotator
        self.system_instruction = system_instruction
        self.model = GEMINI_MODEL_NAME
        self.max_history_turns = 15

    def _summarize_old_memory(self, history_to_compress: List[Dict]) -> str:
        """è°ƒç”¨ LLM å°†æ—§å¯¹è¯å‹ç¼©æˆæ‘˜è¦"""
        # ... (ä¿æŒåŸæœ‰çš„æ‘˜è¦é€»è¾‘ä¸å˜) ...
        print("ğŸ§  [Memory] æ­£åœ¨å‹ç¼©é•¿æœŸè®°å¿†...")
        text_content = ""
        for h in history_to_compress:
            role = h.get('role', 'unknown')
            text = h.get('parts', [{}])[0].get('text', '')
            text_content += f"{role}: {text}\n"
            
        prompt = f"Summarize this:\n{text_content}" # ç®€åŒ–ç¤ºæ„
        try:
             # è¿™é‡Œä¸ºäº†ç®€åŒ–ï¼Œç›´æ¥è°ƒç”¨ text promptï¼Œä¸æ¶‰åŠå›¾ç‰‡
            summary = self.rotator.call_gemini_with_rotation(
                model_name=self.model,
                contents=[{"role": "user", "parts": [{"text": prompt}]}],
                system_instruction="You are a memory compressor."
            )
            return summary or ""
        except: return ""

    def run(self, state: Dict[str, Any]) -> Dict[str, Any]:
        current_state = state.get("project_state")
        if not current_state: return {}

        # --- [Step 1: è®°å¿†ç®¡ç†] ---
        # (ä¿æŒåŸæœ‰çš„è®°å¿†å‹ç¼©é€»è¾‘)
        if len(current_state.full_chat_history) > self.max_history_turns:
            split_index = len(current_state.full_chat_history) - 5
            to_compress = current_state.full_chat_history[:split_index]
            recent = current_state.full_chat_history[split_index:]
            new_summary = self._summarize_old_memory(to_compress)
            if current_state.long_term_memory:
                current_state.long_term_memory += f"\n[Update]: {new_summary}"
            else:
                current_state.long_term_memory = new_summary
            current_state.full_chat_history = recent
            print(f"ğŸ“‰ [Memory] å†å²å·²æˆªæ–­ã€‚é•¿æœŸè®°å¿†å·²æ›´æ–°ã€‚")

        print(f"\nâš™ï¸ [Orchestrator] æ­£åœ¨åˆ†æé¡¹ç›®çŠ¶æ€ (Supervisor Mode)...")
        
        # --- [Step 2: æ„å»ºå¢å¼ºå‹ Context] ---
        # è¿™é‡Œæˆ‘ä»¬æ„é€  Prompt çš„æ–‡æœ¬éƒ¨åˆ†
        context_str = f"Task: {current_state.user_input}\n"
        
        if current_state.long_term_memory:
            context_str += f"\n=== LONG TERM MEMORY ===\n{current_state.long_term_memory}\n========================\n"
        
        if current_state.user_feedback_queue:
            print(f"ğŸ”” [Orchestrator] æ£€æµ‹åˆ°ç”¨æˆ·å¹²é¢„: {current_state.user_feedback_queue}")
            context_str += f"\nUSER INTERVENTION: {current_state.user_feedback_queue}\n"

        if current_state.last_error:
            context_str += f"Last Error: {current_state.last_error}\n"
            
        artifacts_str = "\nArtifacts:\n"
        if current_state.artifacts:
            for key, data in current_state.artifacts.items():
                if key == "images":
                    artifacts_str += f"- [Images]: {len(data)} image files generated by Coding Crew.\n"
                else:
                    artifacts_str += f"- [{key}]: Data available.\n"
        
        history_summary = []
        for h in current_state.full_chat_history: 
             role = h.get('role', 'unknown')
             parts = h.get('parts', [{'text': ''}])
             text = parts[0].get('text', '') if parts else ''
             history_summary.append(f"{role}: {text[:200]}...")
        history_str = "\n".join(history_summary)

        final_prompt_text = f"""
        Based on the current state, decide the next step.
        {context_str}
        {artifacts_str}
        Current Conversation:
        {history_str}
        """

        # --- [Step 3: æ„é€ å¤šæ¨¡æ€ Payload] ---
        # å¦‚æœçŠ¶æ€ä¸­æœ‰å›¾ç‰‡æ•°æ®ï¼Œæˆ‘ä»¬éœ€è¦å°†å…¶åŠ å…¥åˆ° parts ä¸­
        message_parts = [{"text": final_prompt_text}]
        
        if current_state.image_data:
            print("ğŸ‘ï¸ [Orchestrator] æ£€æµ‹åˆ°å›¾ç‰‡è¾“å…¥ï¼Œå¯ç”¨è§†è§‰åˆ†æèƒ½åŠ›...")
            message_parts.append({
                "inline_data": {
                    "mime_type": "image/jpeg", # ç®€å•èµ·è§å‡è®¾ä¸º JPEGï¼Œå®é™…åº”ä»æ–‡ä»¶å¤´åˆ¤æ–­æˆ–ç”± input ä¼ å…¥
                    "data": current_state.image_data
                }
            })

        try:
            response = self.rotator.call_gemini_with_rotation(
                model_name=self.model,
                contents=[{"role": "user", "parts": message_parts}], # è¿™é‡Œä¼ å…¥åŒ…å«å›¾ç‰‡çš„ parts
                system_instruction=self.system_instruction,
                response_schema=SupervisorDecision
            )
            
            if response:
                if isinstance(response, dict):
                    decision = SupervisorDecision.model_validate(response)
                else:
                    decision = SupervisorDecision.model_validate_json(response)
                    
                print(f"   ğŸ§  å†³ç­–: {decision.next_agent} | åŸå› : {decision.reasoning}")

                if decision.next_agent == "FINISH":
                    current_state.router_decision = "finish"
                    current_state.next_step = None
                    if not current_state.final_report:
                        current_state.final_report = decision.instruction
                else:
                    current_state.router_decision = "continue"
                    current_state.next_step = {
                        "agent_name": decision.next_agent, 
                        "instruction": decision.instruction
                    }
                
                current_state.user_feedback_queue = None
                current_state.last_error = None
                
            else:
                raise ValueError("Orchestrator API returned empty.")

        except Exception as e:
            print(f"âŒ [Orchestrator] Planning Failed: {e}")
            current_state.last_error = str(e)
            current_state.router_decision = "human"

        return {"project_state": current_state}
